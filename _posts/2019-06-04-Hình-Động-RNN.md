---
layout: post
title: Tìm hiểu về RNN, LSTM, và GRU
subtitle: Recurrent Neural Network, Tự học
tags: [neural network]
comments: false
---

### Giới thiệu


Mạng Recurrent Neural Network là một loại khác bên cạnh ConvNet để tạo nên Arifical Neural Network. Hôm nay tôi sẽ tìm hiểu về mạng RNN và ứng dụng của nó thông qua animated blog tôi đã đọc được đâu đó. Bạn sẽ cảm thấy dễ hiểu hơn thông quả các hình động về cách hoạt động cũng như cáu tạo của từng loại RNN.

### Phân loại

Có 3 lọa RNN phổ biến đó là:
- Vanilla RNN
- Long Short-term memory (LSTM) đề xuất năm 1997
- Gated Curent Units (GRU) đề xuất naem 2014

Lưu ý: Ở đây gọi là Vanilla RNN để phân biệt với Recurrent Neural Network nói chung. Ta cũng hiểu Vanilla là lọa RNN đơn giản nhất và cũng có nhiều khuyết điểm nhất. Khuyến điểm tôi ám chỉ là hiện tượng Vanish Gradient hoặc Exploding gradient trong quá trình training loại mạng này.

### Cảm hứng

Bạn có thể tham khảo thêm bài viết [này](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21). Với bài viết có các hình động sẽ giúp chúng ta dễ hình dung bên trong từng mạng neural.

Bài học hôm nay tôi trích xuất highlight nững kiến thức tập trung vào Vanilla RNN, LSTM và GRU unit cell. Nói ngắn gọn và xúc tích cho nhưng ai mới bát đầu làm quen với RNN và muôn nắm bắt được tổng quan về loai mạng này.

![](https://cdn-images-1.medium.com/max/1000/1*kLIkXgfeGRdi1Mds5CV5xA.png)

Đây là những quy ước để bạ có thể theo dõi các hình minh họa. Nó bao gồm các phép toán như nhân theo từng ví trí, cộng theo từng vị trí, trừ theo từng vị trí, phép tính tỏng theo hệ số sử dụng hàm active là sigmoid, phép tính tổng hệ số theo hàm active là tanh. Một lúc nữa cúng ta sẽ thấy vì sao hai hàm này lại được dùng ở đây và có mục đích gì trong RNN. Và cuối cùng là phép gộp (concatenate) stacked là mmotj cách gọi khác.

### Vanilla RNN

Đây là hình động mô phỏng cách hoạt động của vanilla RNN- loại RNN sơ khai và rất rất được dùng vì thực tế là nó rất khó triển khai vì gặp một vấn đề tôi đã nói qua ở phần trên.

![](https://cdn-images-1.medium.com/max/1000/1*xn5kA92_J5KLaKcP7BMRLA.gif)

Tỗi vẫn đem hình này vào nhưng để hiểu rõ hơn về loại này tôi sẽ có một bài viết khác cụ thể hơn.

- t: timestep
- X: input
- h: hidden state
- length ò h: là số hidden unit

Cs thư viện khác nhau thì gọi hidden unit khác nhau: Keras gọi là state_size hoạc units, Pytorch gọi là hidden_size trong khi TensorFlow gọi là num_units. Các bạn lưu ý nhé. Tên gọi có thể khác nhau chút ít nhưng ý nghĩa là như nhau.

### LSTM

Đây là hình đọng mô phỏng của LSTM rất dễ hình dung phải không.

![](https://cdn-images-1.medium.com/max/1000/1*goJVQs-p9kgLODFNyhl9zA.gif)

- C: cell sate

Lưu ý: chiều của cell state và hiden state là như nhau

### GRU

Còn đay là của GRU nhé

![](https://cdn-images-1.medium.com/max/1000/1*FpRS0C3EHQnELVaWRvb8bg.gif)







